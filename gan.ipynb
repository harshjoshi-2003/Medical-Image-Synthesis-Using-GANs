{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import\n",
    "\n",
    "import os\n",
    "import time\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.models import Sequential, Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import Input, Dense, BatchNormalization, Conv2D, Conv2DTranspose, ReLU, LeakyReLU, Flatten, MaxPooling2D, Dropout, Reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU Initialization\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "        tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization\n",
    "\n",
    "BUFFER_SIZE = 16000\n",
    "BATCH_SIZE = 16\n",
    "batch_size = BATCH_SIZE\n",
    "EPOCHS = 30\n",
    "latent_dim = 128\n",
    "input_size = [128, 128, 3]\n",
    "image_size = (128, 128)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    rescale=1./255,   \n",
    ")\n",
    "\n",
    "image_directory = 'C:\\\\Users\\\\HARSH\\\\OneDrive\\\\Desktop\\\\GAN Project\\\\dataset_download\\\\chest_xray'\n",
    "\n",
    "dataset= datagen.flow_from_directory(\n",
    "    os.path.join(image_directory, 'train'),   \n",
    "    classes=['PNEUMONIA'],   \n",
    "    target_size=image_size,        \n",
    "    batch_size=BATCH_SIZE,      \n",
    "    class_mode='binary',        \n",
    "    shuffle=True                 \n",
    ")\n",
    "\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator Model\n",
    "\n",
    "def gen_model():\n",
    "    model = Sequential([\n",
    "        Input(shape=(latent_dim,)),\n",
    "        Dense(8 * 8 * 128),  # Reduced number of units in the Dense layer\n",
    "        Reshape((8, 8, 128)),\n",
    "        Conv2DTranspose(128, kernel_size=4, strides=2, padding='same'), \n",
    "        LeakyReLU(alpha=0.1),\n",
    "        Conv2DTranspose(128, kernel_size=4, strides=2, padding='same'),\n",
    "        LeakyReLU(alpha=0.1),\n",
    "        Conv2DTranspose(128, kernel_size=4, strides=2, padding='same'),  # Reduced number of filters\n",
    "        LeakyReLU(alpha=0.1),\n",
    "        Conv2DTranspose(128, kernel_size=4, strides=2, padding='same'),\n",
    "        LeakyReLU(alpha=0.1),\n",
    "        Conv2D(3, kernel_size=4, padding='same', activation='sigmoid')\n",
    "    ], name=\"generator\")    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discriminator Model\n",
    "\n",
    "def disc_model():\n",
    "    model = Sequential([\n",
    "        Input(shape=input_size),\n",
    "        Conv2D(128, kernel_size=4, strides=2, padding='same'),  # Reduced number of filters\n",
    "        BatchNormalization(),\n",
    "        LeakyReLU(alpha=0.1),\n",
    "        MaxPooling2D(strides=2),\n",
    "        Conv2D(128, kernel_size=4, strides=2, padding='same'),  # Reduced number of filters\n",
    "        BatchNormalization(),\n",
    "        LeakyReLU(alpha=0.1),\n",
    "        MaxPooling2D(strides=2),\n",
    "        Conv2D(128, kernel_size=4, strides=2, padding='same'),  # Reduced number of filters\n",
    "        BatchNormalization(),\n",
    "        LeakyReLU(alpha=0.1),\n",
    "        Conv2D(128, kernel_size=4, strides=2, padding='same'),  # Reduced number of filters\n",
    "        BatchNormalization(),\n",
    "        LeakyReLU(alpha=0.1),\n",
    "        MaxPooling2D(strides=2),\n",
    "        \n",
    "        Flatten(),\n",
    "        Dense(128),  # Reduced size of the dense layer\n",
    "        LeakyReLU(alpha=0.1),\n",
    "        Dropout(0.2),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ], name=\"discriminator\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = gen_model()\n",
    "discriminator = disc_model()\n",
    "\n",
    "generator.summary()\n",
    "discriminator.summary()\n",
    "\n",
    "\n",
    "#helper funtion to help us with loadidng images in batches\n",
    "def image_loader(generator):\n",
    "    for images, labels in generator:\n",
    "        yield images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GAN with Custom Traning Step\n",
    "\n",
    "#gan model with custom gradient calculation\n",
    "class Gan(Model):\n",
    "    def __init__(self, discriminator, generator, latent_dim):\n",
    "        super().__init__()\n",
    "        self.discriminator = discriminator\n",
    "        self.generator = generator\n",
    "        self.latent_dim = latent_dim\n",
    "    \n",
    "    def compile(self, disc_opt, gen_opt, loss_function):\n",
    "        super().compile()\n",
    "        self.disc_opt = disc_opt\n",
    "        self.gen_opt = gen_opt\n",
    "        self.loss_function = loss_function\n",
    "        self.disc_loss_metric = tf.keras.metrics.Mean(name = \"disc_loss\")\n",
    "        self.gen_loss_metric = tf.keras.metrics.Mean(name = \"gen_loss\")\n",
    "        \n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [self.disc_loss_metric, self.gen_loss_metric]\n",
    "    \n",
    "    def train_step(self, data):  # Modify the function to accept labels separately\n",
    "        real_images, real_labels = data\n",
    "        batch_size = tf.shape(real_images)[0]\n",
    "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
    "\n",
    "        # Fake image decoding\n",
    "        generated_images = self.generator(random_latent_vectors)\n",
    "        combined_images = tf.concat([generated_images, real_images], axis=0)\n",
    "\n",
    "        # Concatenate the real and fake labels\n",
    "        labels = tf.concat(\n",
    "            [tf.ones((batch_size, 1)), tf.zeros((batch_size, 1))], axis=0\n",
    "        )\n",
    "        labels += 0.05*tf.random.uniform(tf.shape(labels))\n",
    "        \n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = self.discriminator(combined_images)\n",
    "            disc_loss = self.loss_function(labels, predictions)    \n",
    "        grads  = tape.gradient(disc_loss, self.discriminator.trainable_weights)\n",
    "        self.disc_opt.apply_gradients(\n",
    "            zip(grads, self.discriminator.trainable_weights)\n",
    "        )\n",
    "        \n",
    "        random_latent_vectors = tf.random.normal(shape = (batch_size,self.latent_dim))\n",
    "        misleading_labels = tf.zeros((batch_size, 1))\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = self.discriminator(self.generator(random_latent_vectors))    \n",
    "            gen_loss = self.loss_function(misleading_labels, predictions)\n",
    "        grads = tape.gradient(gen_loss, self.generator.trainable_weights)\n",
    "        self.gen_opt.apply_gradients(zip(grads, self.generator.trainable_weights))\n",
    "        \n",
    "        self.disc_loss_metric.update_state(disc_loss)\n",
    "        self.gen_loss_metric.update_state(gen_loss)\n",
    "        return{\n",
    "            \"disc_loss\": self.disc_loss_metric.result(),\n",
    "            \"gen_loss\": self.gen_loss_metric.result()\n",
    "        }\n",
    "\n",
    "def gen_images(generator, current_epoch, latent_dim, num_samples=2):\n",
    "    noise = tf.random.normal([num_samples, latent_dim], dtype=tf.float32)\n",
    "    generated_images = generator(noise, training=False)\n",
    "\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(20, 20))\n",
    "    for i, ax in enumerate(axs.flatten()):\n",
    "        ax.imshow(generated_images[i, :, :, 0], cmap='gray')\n",
    "        ax.set_title(f\"After epoch {current_epoch}\")\n",
    "        ax.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'After_epochs_{current_epoch:04d}.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks \n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "#callbacks. We are showing progress of gan and also saving samples after each epochs\n",
    "class Gan_Callback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, num_images=2, latent_dim = 128):\n",
    "        self.num_images = num_images\n",
    "        self.latent_dim = latent_dim       \n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs =None):\n",
    "        latent_vectors = tf.random.normal(shape = (self.num_images, latent_dim))\n",
    "        generated_images = self.model.generator(latent_vectors)\n",
    "        generated_images *=255\n",
    "        generated_images.numpy()\n",
    "        figure = plt.figure(figsize=(10,10))\n",
    "        for i in range(generated_images.shape[0]):\n",
    "            plt.subplot(2, 2,i+1)\n",
    "            plt.imshow(generated_images[i, :, :, 0, ], cmap='gray')\n",
    "            plt.title(f\"After epoch {epoch+1}\")\n",
    "            plt.axis('off')\n",
    "        plt.savefig('After epochs{:04d}.png'.format(epoch+1))\n",
    "        plt.show()\n",
    "        if(epoch % 10 ==0):\n",
    "            self.model.generator.save('/content/drive/MyDrive/gen.h5')\n",
    "            self.model.discriminator.save('/content/drive/MyDrive/disc.h5')\n",
    "\n",
    "gan = Gan(discriminator=discriminator, generator=generator, latent_dim=latent_dim)\n",
    "gan.compile(\n",
    "        disc_opt=tf.keras.optimizers.Adam(learning_rate=0.0002, beta_1=0.5), \n",
    "        gen_opt=tf.keras.optimizers.Adam(learning_rate=0.0002, beta_1=0.5),\n",
    "        # Parallel gpu computing won't work unless  we pass reduction=tf.keras.losses.Reduction.NONE as a parameter too.\n",
    "        loss_function=tf.keras.losses.BinaryCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.NONE),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Traning\n",
    "\n",
    "#actual traing begins here\n",
    "history = gan.fit(\n",
    "    image_loader(dataset), \n",
    "    epochs=EPOCHS,\n",
    "    steps_per_epoch=len(dataset),  \n",
    "    callbacks=[Gan_Callback(num_images=4, latent_dim=latent_dim)]\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
